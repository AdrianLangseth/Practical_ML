{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "581f1dc0",
   "metadata": {},
   "source": [
    "# Concise Repeatable Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4a6c0",
   "metadata": {},
   "source": [
    "# Content\n",
    "1. [H2O AutoML](#H2O-AutoML)\n",
    "1. [Stacking](#Stacking)\n",
    "1. [Blending](#Blending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e4fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.core.display import display, HTML\n",
    "from plotly import tools\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Markdown\n",
    "import scipy.stats as ss\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import category_encoders as ce\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.metrics import auc,plot_roc_curve\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede34f7d",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "\n",
    "Our best is a blend of H2O AutoML and a stacked Catboost and HistGradientBoostingClassifier. This will be done in two parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a400d77d",
   "metadata": {},
   "source": [
    "# H2O AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9a7665",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f0564",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../tdt05-2021-challenge-2/challenge2_train.csv')\n",
    "test = pd.read_csv('../tdt05-2021-challenge-2/challenge2_test.csv')\n",
    "\n",
    "ord_features = ['f1', 'f2', 'f3', 'f5', 'f7', 'f10', 'f13', 'f18', 'f19', 'f27']\n",
    "num_ords = ['f3', 'f5', 'f7', 'f19', 'f27']\n",
    "alpha_ords = ['f1', 'f2', 'f10', 'f18']\n",
    "\n",
    "numeric = ['f11', 'f17', 'f24', 'f28']\n",
    "bell_curve = ['f11', 'f28']\n",
    "long_tail = ['f17', 'f24']  # F24 has -1 as null values. Remove before scaling\n",
    "\n",
    "bin_features = ['f0', 'f4', 'f6', 'f25', 'f26']\n",
    "cyc_features = ['f16', 'f21']\n",
    "nom_features = ['f8', 'f9', 'f12', 'f14', 'f15', 'f22', 'f23']  # hexes\n",
    "duplicate = ['f20']\n",
    "\n",
    "all_feat=bin_features+nom_features+ord_features+['f16_sin', 'f16_cos', 'f21_sin', 'f21_cos']+numeric\n",
    "\n",
    "categorical = bin_features+nom_features+ord_features+ ['f1_0', 'f1_1']+['f16_sin', 'f16_cos', 'f21_sin', 'f21_cos']\n",
    "\n",
    "#Split up the dub\n",
    "train['f1_0'] = train['f1'].apply(lambda x: x[0] if type(x) is str else x)\n",
    "test['f1_0'] = test['f1'].apply(lambda x: x[0] if type(x) is str else x)\n",
    "train['f1_1'] = train['f1'].apply(lambda x: x[1] if type(x) is str else x)\n",
    "test['f1_1'] = test['f1'].apply(lambda x: x[1] if type(x) is str else x)\n",
    "train.drop(['f1'], axis=1, inplace=True)\n",
    "test.drop(['f1'], axis=1, inplace=True)\n",
    "ord_features.remove('f1')\n",
    "ord_features.extend(['f1_0', 'f1_1'])\n",
    "\n",
    "# EHM something\n",
    "\n",
    "train[['f5']].replace(-1.0, np.nan, inplace=True)\n",
    "test[['f5']].replace(-1.0, np.nan, inplace=True)\n",
    "\n",
    "# Convert those alphas which are very indicative\n",
    "\n",
    "train['f13'] = train['f13'].apply(lambda x: (ord(x) - ord('a'))/14 if pd.notnull(x) else x)\n",
    "test['f13'] = test['f13'].apply(lambda x: (ord(x) - ord('a'))/14 if pd.notnull(x) else x)\n",
    "ord_features.remove('f13')\n",
    "\n",
    "train['f10'] = train['f10'].apply(lambda x: (ord(x) - ord('A'))/25 if pd.notnull(x) else x)\n",
    "test['f10'] = test['f10'].apply(lambda x: (ord(x) - ord('A'))/25 if pd.notnull(x) else x)\n",
    "ord_features.remove('f10')\n",
    "\n",
    "\n",
    "## ChooChoo here comes the number soup\n",
    "scl = preprocessing.StandardScaler()\n",
    "train[['f11', 'f28']] = scl.fit_transform(train[['f11', 'f28']])\n",
    "test[['f11', 'f28']] = scl.transform(test[['f11', 'f28']])\n",
    "    \n",
    "train[['f24']].replace(-1.0, np.nan, inplace=True)\n",
    "test[['f24']].replace(-1.0, np.nan, inplace=True)\n",
    "train[['f17', 'f24']] = np.log(train[['f17', 'f24']])\n",
    "test[['f17', 'f24']] = np.log(test[['f17', 'f24']])\n",
    "\n",
    "scl = preprocessing.StandardScaler()\n",
    "train[['f17', 'f24']] = scl.fit_transform(train[['f17', 'f24']])\n",
    "test[['f17', 'f24']] = scl.transform(test[['f17', 'f24']])\n",
    "\n",
    "# 9 million bicycles in Beijing\n",
    "\n",
    "train['f16_sin'] = np.sin((train['f16'] - 1) * (2. * np.pi / 12))\n",
    "train['f16_cos'] = np.cos((train['f16'] - 1) * (2. * np.pi / 12))\n",
    "test['f16_sin'] = np.sin((test['f16'] - 1) * (2. * np.pi / 12))\n",
    "test['f16_cos'] = np.cos((test['f16'] - 1) * (2. * np.pi / 12))\n",
    "train.drop(['f16'], axis=1, inplace=True)\n",
    "test.drop(['f16'], axis=1, inplace=True)\n",
    "\n",
    "train['f21_sin'] = np.sin((train['f21'] - 1) * (2. * np.pi / 7))\n",
    "train['f21_cos'] = np.cos((train['f21'] - 1) * (2. * np.pi / 7))\n",
    "test['f21_sin'] = np.sin((test['f21'] - 1) * (2. * np.pi / 7))\n",
    "test['f21_cos'] = np.cos((test['f21'] - 1) * (2. * np.pi / 7))\n",
    "train.drop(['f21'], axis=1, inplace=True)\n",
    "test.drop(['f21'], axis=1, inplace=True)\n",
    "    \n",
    "train['f16_sin'].fillna(0, inplace=True)\n",
    "train['f16_cos'].fillna(0, inplace=True)\n",
    "test['f16_sin'].fillna(0, inplace=True)\n",
    "test['f16_cos'].fillna(0, inplace=True)\n",
    "\n",
    "train['f21_sin'].fillna(0, inplace=True)\n",
    "train['f21_cos'].fillna(0, inplace=True)\n",
    "test['f21_sin'].fillna(0, inplace=True)\n",
    "test['f21_cos'].fillna(0, inplace=True)\n",
    "\n",
    "train['f19'] = train['f19'].apply(lambda x: int(x*10) if pd.notnull(x) else x)\n",
    "test['f19'] = test['f19'].apply(lambda x: int(x*10) if pd.notnull(x) else x)\n",
    "\n",
    "target = train['target']\n",
    "test_id = test['id']\n",
    "#train.drop(['target', 'id', 'f20'], axis=1, inplace=True)\n",
    "#test.drop(['id', 'f20'], axis=1, inplace=True)\n",
    "train.drop(['id', 'f20'], axis=1, inplace=True)\n",
    "test.drop(['id', 'f20'], axis=1, inplace=True)\n",
    "\n",
    "categorical = bin_features+nom_features+ord_features\n",
    "h_train = h2o.H2OFrame(train)\n",
    "h_test = h2o.H2OFrame(test)\n",
    "\n",
    "for col in categorical:\n",
    "    h_train[col] = h_train[col].asfactor()\n",
    "    h_test[col] = h_test[col].asfactor()\n",
    "\n",
    "\n",
    "h_train['target'] = h_train['target'].asfactor()\n",
    "\n",
    "y = \"target\"\n",
    "x = h_train.columns\n",
    "x.remove(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3353be3",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aml = H2OAutoML(max_models = 20, seed = 42)\n",
    "aml.train(x = x, y = y, training_frame = h_train)\n",
    "lb = aml.leaderboard\n",
    "lb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b9fb82",
   "metadata": {},
   "source": [
    "## Predict & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa05a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = h2o.as_list(aml.predict(h_test))['p1']\n",
    "\n",
    "path='../tdt05-2021-challenge-2/'\n",
    "submission=pd.read_csv(path+'sample_submission.csv')\n",
    "\n",
    "submission['target'] = preds\n",
    "submission.to_csv('results/automl.csv', index=None)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d20090",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7080dcbb",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d70b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='../tdt05-2021-challenge-2/'\n",
    "train=pd.read_csv(path+'challenge2_train.csv')\n",
    "test=pd.read_csv(path+'challenge2_test.csv')\n",
    "submission=pd.read_csv(path+'sample_submission.csv')\n",
    "\n",
    "ord_features = ['f1', 'f2', 'f3', 'f5', 'f7', 'f10', 'f13', 'f18', 'f19', 'f27']\n",
    "num_ords = ['f3', 'f5', 'f7', 'f19', 'f27']\n",
    "alpha_ords = ['f1', 'f2', 'f10', 'f18']\n",
    "\n",
    "numeric = ['f11', 'f17', 'f24', 'f28']\n",
    "bell_curve = ['f11', 'f28']\n",
    "long_tail = ['f17', 'f24']  # F24 has -1 as null values. Remove before scaling\n",
    "\n",
    "bin_features = ['f0', 'f4', 'f6', 'f25', 'f26']\n",
    "cyc_features = ['f16', 'f21']\n",
    "nom_features = ['f8', 'f9', 'f12', 'f14', 'f15', 'f22', 'f23']  # hexes\n",
    "duplicate = ['f20']\n",
    "\n",
    "\n",
    "all_feat=bin_features+nom_features+ord_features+['f16_sin', 'f16_cos', 'f21_sin', 'f21_cos']+numeric\n",
    "\n",
    "# CREDITS : https://www.kaggle.com/caesarlupum/2020-20-lines-target-encoding\n",
    "\n",
    "def encoding(train, test, smooth):\n",
    "    print('Target encoding...')\n",
    "    train.sort_index(inplace=True)\n",
    "    target = train['target']\n",
    "    test_id = test['id']\n",
    "    train.drop(['target', 'id', 'f20'], axis=1, inplace=True)\n",
    "    test.drop(['id', 'f20'], axis=1, inplace=True)\n",
    "    \n",
    "    #Split up the dub\n",
    "    train['f1_0'] = train['f1'].apply(lambda x: x[0] if type(x) is str else x)\n",
    "    test['f1_0'] = test['f1'].apply(lambda x: x[0] if type(x) is str else x)\n",
    "    train['f1_1'] = train['f1'].apply(lambda x: x[1] if type(x) is str else x)\n",
    "    test['f1_1'] = test['f1'].apply(lambda x: x[1] if type(x) is str else x)\n",
    "    train.drop(['f1'], axis=1, inplace=True)\n",
    "    test.drop(['f1'], axis=1, inplace=True)\n",
    "    ord_features.remove('f1')\n",
    "    ord_features.extend(['f1_0', 'f1_1'])\n",
    "    \n",
    "    train[['f5']].replace(-1.0, np.nan, inplace=True)\n",
    "    test[['f5']].replace(-1.0, np.nan, inplace=True)\n",
    "    \n",
    "    # Convert the alphas which are VERY indicative\n",
    "    \n",
    "    train['f13'] = train['f13'].apply(lambda x: (ord(x) - ord('a')) if pd.notnull(x) else x)\n",
    "    test['f13'] = test['f13'].apply(lambda x: (ord(x) - ord('a')) if pd.notnull(x) else x)\n",
    "    ord_features.remove('f13')\n",
    "    \n",
    "    train['f10'] = train['f10'].apply(lambda x: (ord(x) - ord('A')) if pd.notnull(x) else x)\n",
    "    test['f10'] = test['f10'].apply(lambda x: (ord(x) - ord('A')) if pd.notnull(x) else x)\n",
    "    ord_features.remove('f10')\n",
    "    \n",
    "    \n",
    "    ## ChooChoo here comes the number soup\n",
    "    scl = preprocessing.StandardScaler()\n",
    "    train[['f11', 'f28']] = scl.fit_transform(train[['f11', 'f28']])\n",
    "    test[['f11', 'f28']] = scl.transform(test[['f11', 'f28']])\n",
    "    \n",
    "    train[['f24']].replace(-1.0, np.nan, inplace=True)\n",
    "    test[['f24']].replace(-1.0, np.nan, inplace=True)\n",
    "\n",
    "    train[['f17', 'f24']] = np.log(train[['f17', 'f24']])\n",
    "    test[['f17', 'f24']] = np.log(test[['f17', 'f24']])\n",
    "\n",
    "    scl = preprocessing.StandardScaler()\n",
    "    train[['f17', 'f24']] = scl.fit_transform(train[['f17', 'f24']])\n",
    "    test[['f17', 'f24']] = scl.transform(test[['f17', 'f24']])\n",
    "    \n",
    "    # Push cyclical features onto a unit-circle.\n",
    "    train['f16_sin'] = np.sin((train['f16'] - 1) * (2. * np.pi / 12))\n",
    "    train['f16_cos'] = np.cos((train['f16'] - 1) * (2. * np.pi / 12))\n",
    "\n",
    "    test['f16_sin'] = np.sin((test['f16'] - 1) * (2. * np.pi / 12))\n",
    "    test['f16_cos'] = np.cos((test['f16'] - 1) * (2. * np.pi / 12))\n",
    "\n",
    "    train.drop(['f16'], axis=1, inplace=True)\n",
    "    test.drop(['f16'], axis=1, inplace=True)\n",
    "    \n",
    "    train['f21_sin'] = np.sin((train['f21'] - 1) * (2. * np.pi / 7))\n",
    "    train['f21_cos'] = np.cos((train['f21'] - 1) * (2. * np.pi / 7))\n",
    "\n",
    "    test['f21_sin'] = np.sin((test['f21'] - 1) * (2. * np.pi / 7))\n",
    "    test['f21_cos'] = np.cos((test['f21'] - 1) * (2. * np.pi / 7))\n",
    "\n",
    "    train.drop(['f21'], axis=1, inplace=True)\n",
    "    test.drop(['f21'], axis=1, inplace=True)\n",
    "    \n",
    "    train['f16_sin'].fillna(0, inplace=True)\n",
    "    train['f16_cos'].fillna(0, inplace=True)\n",
    "    test['f16_sin'].fillna(0, inplace=True)\n",
    "    test['f16_cos'].fillna(0, inplace=True)\n",
    "\n",
    "    train['f21_sin'].fillna(0, inplace=True)\n",
    "    train['f21_cos'].fillna(0, inplace=True)\n",
    "    test['f21_sin'].fillna(0, inplace=True)\n",
    "    test['f21_cos'].fillna(0, inplace=True)\n",
    "    \n",
    "    cyc_features = ['f16_sin', 'f16_cos', 'f21_sin', 'f21_cos']\n",
    "    # ord_features.remove('f16')\n",
    "    cat_feat = bin_features+nom_features+ord_features+cyc_features\n",
    "    smoothing=smooth\n",
    "    oof = pd.DataFrame([])\n",
    "    \n",
    "    for tr_idx, oof_idx in StratifiedKFold(n_splits=5, random_state=42, shuffle=True).split(train, target):\n",
    "        ce_target_encoder = ce.TargetEncoder(cols = cat_feat, smoothing=smoothing)\n",
    "        ce_target_encoder.fit(train.iloc[tr_idx, :], target.iloc[tr_idx])\n",
    "        oof = oof.append(ce_target_encoder.transform(train.iloc[oof_idx, :]), ignore_index=False)\n",
    "        \n",
    "    ce_target_encoder = ce.TargetEncoder(cols = cat_feat, smoothing=smoothing)\n",
    "    ce_target_encoder.fit(train, target)\n",
    "    train = oof.sort_index()\n",
    "    test = ce_target_encoder.transform(test)\n",
    "    print('Done!')\n",
    "    return train, test, test_id, list(train), target\n",
    "\n",
    "all_feat.remove('f1')\n",
    "all_feat.extend(['f1_0', 'f1_1'])\n",
    "\n",
    "train_encode, test_encode, test_id, features, target = encoding(train, test, 0.3)\n",
    "train_encode=pd.concat([train_encode,target],axis=1,ignore_index=True)\n",
    "train_encode.columns=list(train.columns)+['target']\n",
    "\n",
    "X, y = train_encode[all_feat], train_encode['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0229298c",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bf87fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kitty():\n",
    "    clf = CatBoostClassifier(\n",
    "                               loss_function='CrossEntropy',\n",
    "                               eval_metric=\"AUC\",\n",
    "                               task_type=\"CPU\",\n",
    "                               learning_rate=0.015,\n",
    "                               n_estimators =2000,\n",
    "                               early_stopping_rounds=100,\n",
    "                               random_seed=42,\n",
    "                               silent=True\n",
    "                              )\n",
    "        \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d264a2",
   "metadata": {},
   "source": [
    "## Back To Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f14c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = \"roc_auc\"\n",
    "\n",
    "HistGBM_param = {\n",
    "    'l2_regularization': 0.0,\n",
    "    'loss': 'auto',\n",
    "    'max_bins': 255,\n",
    "    'max_depth': 15,\n",
    "    'max_leaf_nodes': 31,\n",
    "    'min_samples_leaf': 20,\n",
    "    'n_iter_no_change': 50,\n",
    "    'scoring': scoring,\n",
    "    'tol': 1e-07,\n",
    "    'validation_fraction': 0.15,\n",
    "    'verbose': 0,\n",
    "    'warm_start': False   \n",
    "}\n",
    "\n",
    "\n",
    "xgBoost_params = {\n",
    "    'verbose': 0,\n",
    "    'scoring':  scoring,\n",
    "    'eval_metric': ['auc'],\n",
    "    'objective': 'binary:logistic',\n",
    "    'learning_rate': 0.15, \n",
    "    'max_depth': 2, \n",
    "    'subsample': 0.7,\n",
    "    'min_child_weight': 500, \n",
    "    'colsample_bytree': 0.2, \n",
    "    'reg_lambda': 3.5, \n",
    "    'reg_alpha': 1.5,\n",
    "    'num_parallel_tree': 5,\n",
    "    'n_estimators': 200,\n",
    "    'early_stopping_rounds': 100\n",
    "}\n",
    "\n",
    "\n",
    "folds = StratifiedKFold(n_splits=7, shuffle=True, random_state=1)\n",
    "fold_preds = np.zeros([test_encode.shape[0],3])\n",
    "oof_preds = np.zeros([X.shape[0],3])\n",
    "results = {}\n",
    "\n",
    "estimators = [\n",
    "        ('histgbm', HistGradientBoostingClassifier(**HistGBM_param)),\n",
    "        ('catboost', make_kitty())\n",
    "        # ('xgboost', XGBClassifier(**xgBoost_params))\n",
    "    ]\n",
    "\n",
    "# Fit Folds\n",
    "for i, (trn_idx, val_idx) in enumerate(folds.split(X,y)):\n",
    "    print(f\"Fold {i} stacking....\")\n",
    "    clf = StackingClassifier(\n",
    "            estimators=estimators,\n",
    "            final_estimator=LogisticRegression(),\n",
    "            )\n",
    "    clf.fit(X.loc[trn_idx,:], y.loc[trn_idx])\n",
    "    tmp_pred = clf.predict_proba(X.loc[val_idx,:])[:,1]\n",
    "    \n",
    "    oof_preds[val_idx,0] = tmp_pred\n",
    "    fold_preds[:,0] += clf.predict_proba(test_encode)[:,1] / folds.n_splits\n",
    "        \n",
    "    estimator_performance = {}\n",
    "    estimator_performance['stack_score'] = metrics.roc_auc_score(y.loc[val_idx], tmp_pred)\n",
    "    \n",
    "    for ii, est in enumerate(estimators):\n",
    "            model = clf.named_estimators_[est[0]]\n",
    "            pred = model.predict_proba(X.loc[val_idx,:])[:,1]\n",
    "            oof_preds[val_idx, ii+1] = pred\n",
    "            fold_preds[:,ii+1] += model.predict_proba(test_encode)[:,1] / folds.n_splits\n",
    "            estimator_performance[est[0]+\"_score\"] = metrics.roc_auc_score(y.loc[val_idx], pred)\n",
    "            \n",
    "    stack_coefficients = {x+\"_coefficient\":y for (x,y) in zip([x[0] for x in estimators], clf.final_estimator_.coef_[0])}\n",
    "    stack_coefficients['intercept'] = clf.final_estimator_.intercept_[0]\n",
    "        \n",
    "    results[\"Fold {}\".format(str(i+1))] = [\n",
    "            estimator_performance,\n",
    "            stack_coefficients\n",
    "        ]\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1583e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['Fold 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f54fd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError()\n",
    "submission['target'] =fold_preds[:,0]\n",
    "submission.to_csv('results/all_feats_13_and_11_asnum_hyp_cat_cyclicals_nans_fixed_7_folds.csv', index=None)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b92bf",
   "metadata": {},
   "source": [
    "# Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba51d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=pd.read_csv(path+'sample_submission.csv')\n",
    "own = pd.read_csv('results/all_feats_13_and_11_asnum_hyp_cat_cyclicals_nans_fixed_7_folds.csv')\n",
    "aml = pd.read_csv('results/automl.csv')\n",
    "submission['target'] = (aml.target*0.76772 + own.target*.76911)/(.76772+.76911)\n",
    "submission.to_csv('results/aml_own_equeal_weighted_blend.csv', index=None)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
